---
title: "内存管理"
description: "从虚拟地址空间布局到物理内存分配算法, 系统性解析 Linux 内核的内存管理机制."
---

Linux 内存管理是一个极度精巧的子系统, 它的目标是在保证进程启动速度、运行效率的同时, 最大化利用物理内存资源. 理解内存管理不仅有助于排查 OOM (Out of Memory) 问题, 更是进行系统级调优的基础.

---

## 1. 虚拟内存与地址空间

Linux 并没有让进程直接访问物理内存, 面对每个进程提供了一个连续的、私有的**虚拟地址空间**.

### 1.1 为什么需要虚拟地址?
1.  **隔离性**: 进程间无法直接访问彼此的地址空间, 提高了安全性.
2.  **连续性**: 尽管物理内存可能支离破碎, 但虚拟内存让进程以为自己拥有连续的地址块.
3.  **扩展性**: 虚拟内存可以大于物理内存 (通过分页交换到磁盘).

### 1.2 映射机制: MMU 与页表
虚拟地址到物理地址的转换由硬件 **MMU (Memory Management Unit)** 完成. 为了减少映射表的体积, 内核采用多级页表结构 (通常为 4 级或 5 级), 仅在真正分配内存时才建立对应的页表项.

---

## 2. 物理内存的组织结构

内核对物理内存的管理遵循 **Node -> Zone -> Page** 的层次结构.

### 2.1 NUMA 与 Node
在现代多路服务器中, 通常采用 **NUMA (Non-Uniform Memory Access)** 架构. 物理内存被划分为多个 Node, CPU 访问本地 Node 的内存速度远快于访问远程 Node.

### 2.2 Zone (管理区)
每个 Node 下又被划分为多个 Zone, 常见的包括:
*   **ZONE_DMA**: 供老旧硬件使用的低端地址空间.
*   **ZONE_DMA32**: 64 位系统中 4GB 以下的区域.
*   **ZONE_NORMAL**: 正常分配的内存区域.

### 2.3 Page (页)
内存管理的最小单位是 **页**, 在 x86_64 架构下默认为 **4KB**.

---

## 3. 内存分配算法

为了解决碎片化问题, Linux 采用了两种互补的分配算法.

### 3.1 伙伴系统 (Buddy System)
*   **解决问题**: 外部碎片 (External Fragmentation).
*   **原理**: 它将物理内存维护成多组大小为 2^n 页的连续块. 申请内存时, 寻找最接近的块; 释放内存时, 检查相邻的块是否空闲并尝试合并.

### 3.2 Slab / Slub 分配器
*   **解决问题**: 内部碎片 (Internal Fragmentation).
*   **原理**: 伙伴系统以页为单位, 对于几十个字节的小对象 (如文件描述符、dentry) 会造成巨大浪费. Slab 分配器预先从伙伴系统申请大块内存, 并将其切分成固定大小的小槽供内核对象使用.

---

## 4. 统计指标: VSS, RSS, PSS 与 USS

在进行内存排障时, 仅看 top 的内存占用是不够的, 必须理解以下四个指标:

| 指标 | 全称 | 说明 |
| :--- | :--- | :--- |
| **VSS** | Virtual Set Size | 虚拟耗用内存 (包含所有库链接、映射但不一定占用的内存). |
| **RSS** | Resident Set Size | 实际驻留内存 (进程真正占用的物理内存, 但包含共享库). |
| **PSS** | Proportional Set Size | 比例驻留内存 (将共享库平摊给所有使用的进程, 更具参考价值). |
| **USS** | Unique Set Size | 独占驻留内存 (进程私有的内存, 杀死该进程后系统能释放的量). |

---

## 5. 内存回收与 OOM Killer

### 5.1 内存过量分配 (Overcommit)
Linux 允许进程申请超过实际物理内存大小的内存. 这是基于 "并非所有进程都会同时用完申请内存" 的假设.
*   相关的内核参数: `vm.overcommit_memory`.

### 5.2 OOM Killer
当系统内存耗尽, 甚至无法满足内核基本运行时, **OOM Killer** 会被启动.
*   **打分机制**: 内核根据进程消耗的内存、运行时间、优先级 (oom_score_adj) 计算出一个 `oom_score`.
*   **策略**: 分数最高的进程将被强行杀死, 以回收空间保证系统存活.

---

## 6. 高级内存特性

### 6.1 大页 (HugePages)

在处理数十 GB 甚至 TB 级别的内存时, 标准的 4KB 页会导致显著的性能瓶颈, 核心原因在于 **TLB (Translation Lookaside Buffer)** 的开销.

*   **TLB 的角色**: TLB 是 CPU 内部的一个高速缓存, 专门用于存放虚拟地址到物理地址的映射关系 (页表项). 由于 CPU 访问内存的速度远慢于寄存器, 如果每次内存操作都要去内存中查询页表, 性能将大幅下降. TLB 缓存了最近使用的映射, 使得地址转换可以在极短时间内完成 (TLB Hit).
*   **4KB 页的挑战**: 对于一个 64GB 内存的数据库应用, 如果使用 4KB 页, 理论上需要维护 1600 万个页表项. 这个数量远远超过了 CPU TLB 的缓存容量, 导致频繁发生 **TLB Miss**. 此时 CPU 必须回退到慢速的内存页表查询 (Page Walk), 极大地增加了指令延迟.
*   **HugePages 的优势**: 通过将页大小提升至 **2MB** 甚至 **1GB**, 同样的内存容量所需的页表项数量减少了数百倍.
    1.  **提高 TLB 命中率**: 较少的条目意味着更多的映射可以驻留在 CPU 缓存中.
    2.  **减少页表级数**: 虚拟地址转换路径变短, 减少了 Page Walk 的深度.
    3.  **减少管理开销**: 降低了内存分配和回收时的锁竞争.

**常见实现方式**:
*   **Static HugePages**: 在系统启动时通过内核参数预留. 它们被锁定在物理内存中, 不会被交换 (Swap) 或回收, 适合数据库 (如 Oracle, PostgreSQL) 的共享内存段.
*   **THP (Transparent HugePages)**: 内核在后台尝试自动合并相邻的小页. 虽然降低了配置门槛, 但在某些高度随机访问的场景下, 后台合并动作 (Compaction) 可能会引发不可监测的 CPU 突发性高负载.

### 6.2 交换分区 (Swap)
Swap 不仅仅是物理内存的备胎, 它更重要的意义在于将那些长期不使用的 "匿名页" 换出, 给活跃的 "文件页 (Page Cache)" 腾出空间, 从而提升整体 I/O 高效性.

---

> 理解内存管理是一场关于 "空间换时间" 与 "精细平衡" 的权衡. 系统性能的优劣, 往往取决于内核如何在快速分配与有效回收之间维持那个动态的平衡点.
